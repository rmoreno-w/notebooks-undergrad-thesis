# Transfer√™ncia de Aprendizado com Dados de √Åudio - TCC

Este projeto foi elaborado durante meu trabalho de conclus√£o de curso e visou investigar a aplica√ß√£o de **transfer√™ncia de aprendizado** para classifica√ß√£o de emo√ß√µes em √°udios, utilizando redes neurais pr√©-treinadas como **YAMNet** e **VGGish**. O objetivo √© comparar o desempenho de diferentes classificadores, como **SVM**, **Random Forest**, **Naive Bayes** e **MLP**, ao classificar emo√ß√µes a partir de dados de √°udio.

O conjunto de dados utilizado √© o **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**, amplamente utilizado para an√°lise de emo√ß√µes em √°udio. Voc√™ pode acessar o conjunto de dados RAVDESS no [link](https://zenodo.org/records/1188976).

> **Note**: If you prefer, you can also read the project description in English üá∫üá∏. Please refer to the [README in english](readme.md) for more details.

## üóÇÔ∏è √çndice

-   [Descri√ß√£o](#descri√ß√£o)
-   [Tecnologias](#tecnologias)
-   [Pr√©-requisitos](#pr√©-requisitos)
-   [Instala√ß√£o](#instala√ß√£o)
-   [Como Usar](#como-usar)
-   [Licen√ßa](#licen√ßa)

## üìú Descri√ß√£o

Este projeto aplica **transfer√™ncia de aprendizado** usando redes neurais pr√©-treinadas (**YAMNet** e **VGGish**) para extrair caracter√≠sticas de √°udio. Em seguida, treinamos classificadores tradicionais (**SVM**, **Random Forest**, **Naive Bayes** e **MLP**) sobre essas caracter√≠sticas extra√≠das, assim como caracter√≠sticas derivadas de **espectrogramas de Mel**, a fim de classificar emo√ß√µes nos √°udios.

O fluxo do projeto √© o seguinte:

1. **Extra√ß√£o de caracter√≠sticas**:

    - **VGGish**: Utiliza um modelo pr√©-treinado para extrair caracter√≠sticas de √°udio.
    - **YAMNet**: Baseado em um modelo de rede neural para extra√ß√£o de caracter√≠sticas de √°udio.
    - **Espectrogramas de Mel**: Extra√ß√£o de espectrogramas de Mel diretamente dos arquivos de √°udio.

2. **Treinamento de Classificadores**:
    - Treinamento de **SVM**, **Random Forest**, **Naive Bayes** e **MLP** para classificar as emo√ß√µes a partir das caracter√≠sticas extra√≠das.

> **Importante**: O script `generate_csv_metadata.py` espera que os arquivos de √°udio estejam em uma pasta chamada `data`. Ele trabalha apenas com os arquivos de **√°udio (n√£o inclui os √°udios dos atores cantando nem os arquivos de v√≠deo)**. Certifique-se de colocar apenas os arquivos de √°udio de fala na pasta `data` para o script funcionar corretamente. Ou modificar o script, se voc√™ preferir fazer de uma maneira diferente.

## üíª Tecnologias

Este projeto foi desenvolvido utilizando

as seguintes bibliotecas e ferramentas:

-   **Python 3.11**
-   **Jupyter Notebooks** (para an√°lise interativa)
-   **Pandas (v 2.2.1)**: Para an√°lise e manipula√ß√£o de dados
-   **Librosa (v 0.10.1)**: Para processamento de arquivos de √°udio
-   **TensorFlow (v 2.13.0)**: Para cria√ß√£o e adapta√ß√£o de redes neurais
-   **TensorFlow Hub (v 0.16.1)**: Para acesso a modelos pr√©-treinados de aprendizado de m√°quina
-   **Scikit-learn (v 1.4.1.post1)**: Para treinamento de modelos de aprendizado de m√°quina tradicionais
-   **Matplotlib (v 3.8.3)**: Para visualiza√ß√£o de resultados e gr√°ficos

## üìå Pr√©-requisitos

Este projeto foi desenvolvido utilizando **Python 3.11** e requer as seguintes bibliotecas:

-   **Pandas** para manipula√ß√£o de dados
-   **Librosa** para leitura e processamento de arquivos de √°udio
-   **TensorFlow** e **TensorFlow Hub** para redes neurais e transfer√™ncia de aprendizado
-   **Scikit-learn** para cria√ß√£o de modelos de aprendizado de m√°quina
-   **Matplotlib** para visualiza√ß√£o de resultados

Um arquivo `requirements.txt` com todas as depend√™ncias pode ser encontrado no reposit√≥rio.

## üõ†Ô∏è Instala√ß√£o

Para rodar este projeto, siga as instru√ß√µes abaixo:

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/seu-usuario/seu-repositorio.git
cd seu-repositorio
```

### 2. Crie e ative um ambiente virtual (opcional, mas recomendado)

```bash
python -m venv venv
```

Ative o ambiente:

-   No Windows:

    ```bash
    venv\Scripts\activate
    ```

-   No Mac/Linux:
    ```bash
    source venv/bin/activate
    ```

### 3. Instale as depend√™ncias

```bash
pip install -r requirements.txt
```

## üßëüèΩ‚Äçüíª Como Usar

### Passo 1: Gerar a tabela CSV com os caminhos dos arquivos e suas emo√ß√µes

Execute o script `generate_csv_metadata.py` para gerar um arquivo CSV contendo os caminhos dos arquivos de √°udio e as emo√ß√µes que eles representam. Este arquivo ser√° utilizado para extra√ß√£o das caracter√≠sticas nos notebooks.

> **Importante**: O script `generate_csv_metadata.py` espera que os arquivos de √°udio estejam localizados em uma pasta chamada `data`. Ele s√≥ processa arquivos de **√°udio (n√£o inclui os √°udios de atores cantando nem arquivos de v√≠deo)**. Coloque apenas os arquivos de √°udio de fala na pasta `data` para o script funcionar corretamente.

```bash
python generate_csv_metadata.py
```

### Passo 2: Extra√ß√£o de Caracter√≠sticas

Ap√≥s gerar o CSV, voc√™ pode usar os notebooks Jupyter para extrair as caracter√≠sticas dos √°udios. Os notebooks est√£o organizados da seguinte forma:

1. **YAMNet**:

    - Abra o notebook `yamnet_feature_extraction.ipynb` para extrair caracter√≠sticas usando o modelo YAMNet.

2. **VGGish**:

    - Abra o notebook `vggish_feature_extraction.ipynb` para extrair caracter√≠sticas usando o modelo VGGish.

3. **Espectrogramas de Mel**:
    - Abra o notebook `mel_spectrogram_extraction.ipynb` para extrair espectrogramas de Mel a partir dos √°udios.

### Passo 3: Treinamento dos Classificadores

Depois de extrair as caracter√≠sticas, voc√™ pode treinar os classificadores usando as features extra√≠das. O c√≥digo para isso est√° nos notebooks, onde voc√™ pode treinar **SVM**, **Random Forest**, **Naive Bayes** e **MLP** para prever a emo√ß√£o nos √°udios.

## üîì Licen√ßa

Este projeto est√° licenciado sob a **Licen√ßa P√∫blica Geral GNU (GPL v3)**. Isso significa que voc√™ √© livre para usar, modificar e distribuir este c√≥digo, desde que tamb√©m o compartilhe sob a mesma licen√ßa. Ou seja, qualquer trabalho derivado tamb√©m precisa ser aberto e licenciado sob a **GPL v3**.

Veja o arquivo [LICENSE](LICENSE) para mais detalhes.
